{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "830fa773-e41d-418d-924b-8bfa7c41fda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import Tool\n",
    "from langchain_community.utilities import GoogleSearchAPIWrapper\n",
    "import requests\n",
    "from newspaper import Article\n",
    "import os\n",
    "\n",
    "# <script async src=\"https://cse.google.com/cse.js?cx=6085da3cd56ea44a4\">\n",
    "# </script>\n",
    "# <div class=\"gcse-search\"></div>\n",
    "os.environ[\"GOOGLE_CSE_ID\"] = \"e5ee12f7668844e83\"\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyBcsiSZTmRWLk5D_TTbAVH87Dl_RPWqXas\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b720698-f58e-4ebf-9fd2-ab8b7c7bfd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "search = GoogleSearchAPIWrapper()\n",
    "\n",
    "def top5_results(query):\n",
    "    return search.results(query, 5)\n",
    "\n",
    "tool = Tool(\n",
    "    name=\"Google Search\",\n",
    "    description=\"Search Google for recent results.\",\n",
    "    func=top5_results,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a797dfd7-973f-4f6e-b155-b83497e49664",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"transformer architecture explained\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3a79ba6e-fbb5-48c8-9c45-bf21c30107cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = tool.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9ece7d69-99fd-4c91-903f-d369ae3dc9de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "link : https://builtin.com/artificial-intelligence/transformer-neural-network \n",
      "\n",
      "\n",
      "The transformer neural network is a novel architecture that aims to solve sequence-to-sequence tasks while handling long-range dependencies with ease. It was first proposed in the paper “Attention Is \n",
      "\n",
      "\n",
      "link : https://medium.com/@amanatulla1606/transformer-architecture-explained-2c49e2257b4c \n",
      "\n",
      "\n",
      "Transformer Architecture explained Amanatullah · Follow 10 min read · Sep 1, 2023 -- 1 Listen Share\n",
      "\n",
      "Transformers are a new development in machine learning that have been making a lot of noise lately.\n",
      "\n",
      "\n",
      "link : https://datagen.tech/guides/computer-vision/transformer-architecture/ \n",
      "\n",
      "\n",
      "What Is a Transformer Model?\n",
      "\n",
      "Transformer is a deep learning (DL) model, based on a self-attention mechanism that weights the importance of each part of the input data differently. It is mainly used i\n",
      "\n",
      "\n",
      "link : http://jalammar.github.io/illustrated-transformer/ \n",
      "\n",
      "\n",
      "Discussions: Hacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\n",
      "\n",
      "Translations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, J\n",
      "\n",
      "\n",
      "link : https://towardsdatascience.com/transformers-141e32e69591 \n",
      "\n",
      "\n",
      "How Transformers Work\n",
      "\n",
      "If you liked this post and want to learn how machine learning algorithms work, how did they arise, and where are they going, I recommend the following:\n",
      "\n",
      "Transformers are a type \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count =0\n",
    "for obj in response:\n",
    "    if \"link\" in obj.keys() :\n",
    "        link = obj.get(\"link\",None)\n",
    "        if link != None:\n",
    "            # f = requests.get(link)\n",
    "            # print(f.text)\n",
    "            print(f\"link : {link} \\n\\n\")\n",
    "\n",
    "            toi_article = Article(link, language=\"en\")\n",
    "            toi_article.download()\n",
    "            toi_article.parse()\n",
    "            toi_article.nlp()\n",
    "            \n",
    "            print(toi_article.text[:200])\n",
    "            count += 1\n",
    "    print(\"\\n\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "25c8303e-fdc3-47fc-bde6-8080f0f67151",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import llms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "51186079-46e5-4c9d-864d-a3d5db1b106e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFaceHub\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3809fbb0-e20e-4220-86f1-99707e6ace26",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0e7fd18b-3610-486b-86a8-0b74747ea55e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kishorekunisetty/opt/anaconda3/envs/instahyre/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'InferenceApi' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '1.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "llm = HuggingFaceHub(\n",
    "    repo_id=\"huggingfaceh4/zephyr-7b-alpha\", \n",
    "    model_kwargs={\"temperature\": 0.9, \"max_length\": 512,\"max_new_tokens\":512}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fa762982-e549-44dd-8813-0794810a8677",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer architecture is a type of neural network designed for sequence processing tasks like natural language processing, speech recognition, and machine translation. This architecture was proposed by Google in 2017 and quickly gained popularity due to its impressive performance on various language processing tasks.\n",
      "\n",
      "Unlike traditional Recurrent Neural Networks (RNNs) that use recurrence to maintain a memory of past inputs, Transformers encapsulate the sequence-to-sequence translation into a self-attention mechanism. Transformers can be trained in parallel and can handle sequences of varying lengths without any recurrent state.\n",
      "\n",
      "In Transformers, the input sequence is passed through a stack of identical layers called Encoder-Decoder layers. Each Encoder-Decoder layer consists of modules like Multi-Head Attention, Position-Wise Feed-Forward Networks (FFNs), and Layer Normalization.\n",
      "\n",
      "The key to understanding Transformers is the concept of self-attention. In self-attention, each input element interacts with every other input element via a learned attention mechanism to create a context for the input sequence.\n",
      "\n",
      "The attention mechanism can be either distributed or local. In the distributed attention mechanism, the attention weights capture global relationships between input elements. This is the case in Transformer-based language models like BERT, which improve performance on sequence classification tasks.\n",
      "\n",
      "In the local attention mechanism, the attention weights capture nearby or short-range dependencies. This is the case in Transformer-based language models like GPT-3, which excel at generating long sequences with high fluency.\n",
      "\n",
      "In conclusion, Transformers are a powerful neural network architecture that can be used to solve sequence processing tasks like natural language processing, speech recognition, and machine translation. The self-attention mechanism within Transformers makes it possible for all input elements to interact with one another, enabling improved performance on sequence classification and generation tasks.\n"
     ]
    }
   ],
   "source": [
    "# query = \"What is capital of India and UAE?\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    " <|system|>\n",
    "You are an AI assistant that follows instruction extremely well.\n",
    "Please be truthful and give direct answers\n",
    "</s>\n",
    " <|user|>\n",
    " {query}\n",
    " </s>\n",
    " <|assistant|>\n",
    "\"\"\"\n",
    "\n",
    "response = llm.predict(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "169296a9-86a8-4580-9810-593c1cb0e293",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d30369e3-4b57-41e4-bf1d-383511d31dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "link : https://builtin.com/artificial-intelligence/transformer-neural-network \n",
      "\n",
      "\n",
      "The transformer neural network is a novel architecture that aims to solve sequence-to-sequence tasks while handling long-range dependencies with ease. It was first proposed in the paper “Attention Is \n",
      "\n",
      "\n",
      "link : https://medium.com/@amanatulla1606/transformer-architecture-explained-2c49e2257b4c \n",
      "\n",
      "\n",
      "Transformer Architecture explained Amanatullah · Follow 10 min read · Sep 1, 2023 -- 1 Listen Share\n",
      "\n",
      "Transformers are a new development in machine learning that have been making a lot of noise lately.\n",
      "\n",
      "\n",
      "link : https://datagen.tech/guides/computer-vision/transformer-architecture/ \n",
      "\n",
      "\n",
      "What Is a Transformer Model?\n",
      "\n",
      "Transformer is a deep learning (DL) model, based on a self-attention mechanism that weights the importance of each part of the input data differently. It is mainly used i\n",
      "\n",
      "\n",
      "link : http://jalammar.github.io/illustrated-transformer/ \n",
      "\n",
      "\n",
      "Discussions: Hacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\n",
      "\n",
      "Translations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, J\n",
      "\n",
      "\n",
      "link : https://towardsdatascience.com/transformers-141e32e69591 \n",
      "\n",
      "\n",
      "How Transformers Work\n",
      "\n",
      "If you liked this post and want to learn how machine learning algorithms work, how did they arise, and where are they going, I recommend the following:\n",
      "\n",
      "Transformers are a type \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Documents = []\n",
    "\n",
    "for obj in response:\n",
    "    if \"link\" in obj.keys() :\n",
    "        link = obj.get(\"link\",None)\n",
    "        if link != None:\n",
    "            print(f\"link : {link} \\n\\n\")\n",
    "\n",
    "            toi_article = Article(link, language=\"en\")\n",
    "            toi_article.download()\n",
    "            toi_article.parse()\n",
    "            toi_article.nlp()\n",
    "\n",
    "            Documents.append(Document(page_content=toi_article.text, metadata={'source': link, 'page_title': toi_article.title}))\n",
    "            \n",
    "            print(toi_article.text[:200])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d41bd2dc-51b9-4565-a359-4a0601ac4d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(separator=' ', chunk_size=1000, chunk_overlap=200)\n",
    "texts = text_splitter.split_documents(Documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b88dd303-938f-414b-b5df-ce20c9a59185",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d368a660-0855-4ad0-ab9d-d0cf2d0806a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings()\n",
    "query_result = embeddings.embed_query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "eaa7721a-6350-45c9-9045-24c07a9bc7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3e8cef1a-28cd-4db7-86f1-ada0545e042f",
   "metadata": {},
   "outputs": [],
   "source": [
    "docsearch = Chroma.from_documents(texts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9741db41-6fd6-4a93-84a7-843ea4110f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import  VectorDBQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "135bcfaf-89a5-4f28-acdd-b1ef1192ac5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kishorekunisetty/opt/anaconda3/envs/instahyre/lib/python3.8/site-packages/langchain/chains/retrieval_qa/base.py:256: UserWarning: `VectorDBQA` is deprecated - please use `from langchain.chains import RetrievalQA`\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Stuff all the information into a single prompt (see https://docs.langchain.com/docs/components/chains/index_related_chains#stuffing)\n",
    "qa = VectorDBQA.from_chain_type(llm=llm, chain_type=\"stuff\", vectorstore=docsearch, return_source_documents=True)\n",
    "result = qa({\"query\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "784ca519-9664-400d-b1a1-02c88a39d21c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'transformer architecture explained',\n",
      " 'result': ' In a nutshell, a transformer model is a deep learning model that '\n",
      "           'uses a self-attention mechanism to process input data '\n",
      "           'sequentially, but unlike recurrent neural networks (RNNs), it '\n",
      "           'processes the entire input at once. The attention mechanism allows '\n",
      "           'the model to focus on the most relevant parts of the input for '\n",
      "           'each output, resulting in parallel processing and a reduction in '\n",
      "           'training time. Transformers have been used successfully for '\n",
      "           'natural language processing tasks such as text summarization and '\n",
      "           'translation, and have recently gained popularity due to the '\n",
      "           'development of large pre-trained systems like BERT.',\n",
      " 'source_documents': [Document(page_content='Transformer Architecture explained Amanatullah · Follow 10 min read · Sep 1, 2023 -- 1 Listen Share\\n\\nTransformers are a new development in machine learning that have been making a lot of noise lately. They are incredibly good at keeping track of context, and this is why the text that they write makes sense. In this chapter, we will go over their architecture and how they work.\\n\\nTransformer models are one of the most exciting new developments in machine learning. They were introduced in the paper Attention is All You Need. Transformers can be used to write stories, essays, poems, answer questions, translate between languages, chat with humans, and they can even pass exams that are hard for humans! But what are they? You’ll be happy to know that the architecture of transformer models is not that complex, it simply is a concatenation of some very useful components, each of which has its own function. In this chapter, you will learn all of these components.\\n\\nIn a nutshell, what does a', metadata={'page_title': 'Transformer Architecture explained', 'source': 'https://medium.com/@amanatulla1606/transformer-architecture-explained-2c49e2257b4c'}),\n",
      "                      Document(page_content='not that complex, it simply is a concatenation of some very useful components, each of which has its own function. In this chapter, you will learn all of these components.\\n\\nIn a nutshell, what does a transformer do? Imagine that you’re writing a text message on your phone. After each word, you may get three words suggested to you. For example, if you type “Hello, how are”, the phone may suggest words such as “you”, or “your” as the next word. Of course, if you continue selecting the suggested word in your phone, you’ll quickly find that the message formed by these words makes no sense. If you look at each set of 3 or 4 consecutive words, it may make sense, but these words don’t concatenate to anything with a meaning. This is because the model used in the phone doesn’t carry the overall context of the message, it simply predicts which word is more likely to come up after the last few. Transformers, on the other hand, keep track of the context of what is being written, and this is why', metadata={'page_title': 'Transformer Architecture explained', 'source': 'https://medium.com/@amanatulla1606/transformer-architecture-explained-2c49e2257b4c'}),\n",
      "                      Document(page_content='But the Transformer consists of six encoders and six decoders.\\n\\nImage from 4\\n\\nEach encoder is very similar to each other. All encoders have the same architecture. Decoders share the same property, i.e. they are also very similar to each other. Each encoder consists of two layers: Self-attention and a feed Forward Neural Network.\\n\\nImage from 4\\n\\nThe encoder’s inputs first flow through a self-attention layer. It helps the encoder look at other words in the input sentence as it encodes a specific word. The decoder has both those layers, but between them is an attention layer that helps the decoder focus on relevant parts of the input sentence.\\n\\nImage from 4\\n\\nSelf-Attention\\n\\nNote: This section comes from Jay Allamar blog post\\n\\nLet’s start to look at the various vectors/tensors and how they flow between these components to turn the input of a trained model into an output. As is the case in NLP applications in general, we begin by turning each input word into a vector using an embedding', metadata={'page_title': 'How Transformers Work. Transformers are a type of neural…', 'source': 'https://towardsdatascience.com/transformers-141e32e69591'}),\n",
      "                      Document(page_content='What Is a Transformer Model?\\n\\nTransformer is a deep learning (DL) model, based on a self-attention mechanism that weights the importance of each part of the input data differently. It is mainly used in computer vision (CV) and natural language processing (NLP).\\n\\nSimilar to recurrent neural networks (RNNs), transformers are designed to process sequential input data like natural language, and perform tasks like text summarization and translation. However, unlike RNNs, transformers process the entire input at once. The attention mechanism allows the model to focus on the most relevant parts of the input for each output.\\n\\nFor instance, if the input data is natural language sentences, the translator will not need to process one word at a time. This makes it possible to parallelize processing, more so than in RNNs, thus reducing training time. This has spurred the development of large pre-trained systems such as Bidirectional Encoder Representations from Transformers (BERT) and Generalized', metadata={'page_title': 'What Is the Transformer Architecture and How Does It Work?', 'source': 'https://datagen.tech/guides/computer-vision/transformer-architecture/'})]}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pprint.pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6eae494b-64f3-42c6-81d8-91c02678227f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' In a nutshell, a transformer model is a deep learning model that uses a self-attention mechanism to process input data sequentially, but unlike recurrent neural networks (RNNs), it processes the entire input at once. The attention mechanism allows the model to focus on the most relevant parts of the input for each output, resulting in parallel processing and a reduction in training time. Transformers have been used successfully for natural language processing tasks such as text summarization and translation, and have recently gained popularity due to the development of large pre-trained systems like BERT.'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "86a91152-770b-4fe8-9b39-8dc96b2ef602",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_links = []\n",
    "for itm in result['source_documents']:\n",
    "    article_links.append(tuple(itm.metadata.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b8ad9a8d-73e0-44a3-8d37-333c6616c6af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('How Transformers Work. Transformers are a type of neural…',\n",
       "  'https://towardsdatascience.com/transformers-141e32e69591'),\n",
       " ('Transformer Architecture explained',\n",
       "  'https://medium.com/@amanatulla1606/transformer-architecture-explained-2c49e2257b4c'),\n",
       " ('What Is the Transformer Architecture and How Does It Work?',\n",
       "  'https://datagen.tech/guides/computer-vision/transformer-architecture/')}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(article_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae1c73f-78e2-45a2-953f-a425431ca1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@st.cache_resource\n",
    "def initiate():\n",
    "    try:\n",
    "        tool = Tool(\n",
    "            name=\"Google Search\",\n",
    "            description=\"Search Google for recent results.\",\n",
    "            func=top5_results,\n",
    "        )\n",
    "\n",
    "        llm = HuggingFaceHub(\n",
    "            repo_id=\"huggingfaceh4/zephyr-7b-alpha\", \n",
    "            model_kwargs={\"temperature\": 0.9, \"max_length\": 512,\"max_new_tokens\":512}\n",
    "        )\n",
    "\n",
    "        text_splitter = CharacterTextSplitter(separator=' ', chunk_size=1000, chunk_overlap=200)\n",
    "        embeddings = HuggingFaceEmbeddings()\n",
    "        print(\"[INFO] >>> initiate success\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        st.write(\"Faced an error initialising resources\")\n",
    "        return False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
